
# Results For Experiment 3

##  Methods
* Languages: Go, Scala, Dart
* Repl.it to manually run and test the code for each langauge
* Google Forms were used for getting feedback from the users.
* Prevented users for accessing Rosetta's code
* The evaluation was based on whether the user was able to find the bug, fix the bug and time taken to fix it.
* Instructions on how to run code for each language in repl.it was provided in the README.md
## Materials
* Repl.it
* Zoom
* Google Forms
* Digital Timers
## Observations

### Mean Debugging Time
![Mean Debugging Time Histogram](Mean_Debug.jfif)
### Proficiency of subjects in all three languages
![User Proficiency Chart](proficiency.jfif)
### Readability of Each language
![Language Readability Chart](readability.jfif)
### Interest in exploring the language further
![Interest in exploring Languages](interest.jfif)

## Conclusions
* Despite taking the most Debugging time for a majority of subjects, users were more interested in exploring Go Language as compared to Dart and Scala. Most subjects also reported Go as the most readable language out of the three. The highest debugging time for Go is accounted for by the fact that one bug in go was more difficult than any other bugs introduced in all the three languages. 
* Dart language was the average of the bunch in terms of debugging time, readability and interest showed. However, interactions with subjects showed that it was the least known language of the three, some of them claiming to hear about the language for the first time during the experiment.
* Although scala was the fastest debugging time by a small margin, it was least readable language for most subjects. Also it was the least popular language to be pursued further. Some users also commented that the slow execution of scala was mildly infuriating to them.

According to our experiments, following is the ranking of the 3 languages, from best to worst : 
1) Go
2) Dart
3) Scala

## Threats to Validity

 - While conducting the experiments, we tried our best to monitor a subject's progress. However, because of the virtual nature of the experiment, it could be that the subjects were collaborating with each other while debugging the code, thus making it easier for them to find and correct faults. Also there were no ways to prevent two subjects from sharing information outside of the experiment.
 -  Our post experiment survey recorded parameters like readability, proficiency etc, which were subjective rather than objective, thus making it difficult to quantify and may cause our results to be misleading.
 - The subjects may have programmed "The Game of Life" in one or more of our selected languages as a part of their group's experiments. This may have caused them to complete the experiment faster than those who hadn't used any of the three previously, given that the logic was derived from the same code base. This discrepancy may have introduced some bias in the experiment results.
 -   Since, the experiment consisted of only 10 test subjects, mostly coming from a similar background (CS grads), the results may have some loopholes. On the contrary, if the user set consisted of (say) 3 college grads, 3 professors and 4 industry level specialists, our results would have been different and much more plausible given the diversity of our subjects. 
